## Описание

Данный крейт является вторым шагом (после создания [токенизатора](https://github.com/Ave-Sergeev/Tokenomicon)) на пути к
полноценной реализации `llm` на архитектуре `transformer`.
Он предназначен для преобразования токенов в эмбединги, а так же для кодирования их позиций.

Глоссарий:

- Токенизация (сегментация) — это процесс разбиения текста на отдельные части (слова, символы, и т.п.)
- LLM (large language models) — это математическая модель общего назначения, предназначенные для широкого спектра задач
  связанных с обработкой естественных языков.
- Transformer — это архитектура глубоких нейронных сетей, представленная в 2017 году исследователями из Google. Она
  предназначена для обработки последовательностей, таких как текст на естественном языке.
- Embedding — это числовые представления текста (токена).
- Positional Encoding — это метод, используемый (например в Transformers), для предоставления модели позиционной
  информации путем добавления зависящих от позиции сигналов к встраиванию слов, что позволяет модели учитывать порядок
  слов во входной последовательности.

### Детали реализации

- Структура Embeddings создает и управляет матрицей эмбеддингов токенов, где каждый столбец представляет токен в
  словаре.
- Структура PositionalEncoding реализует создание и управление кодированием позиций.

Крейт имеет следующие зависимости:

1) [rand](https://github.com/rust-random/rand) крейт для генерации псевдо-случайных значений.
2) [ndarray](https://github.com/rust-ndarray/ndarray) крейт (математический) для эффективной работы с матрицами.
3) [approx](https://github.com/brendanzab/approx) крейт для работы с приближенными сравнениями чисел с плавающей точкой. Используется в тестах.
4) ...

## Использование

Смотреть [пример](/example/src/main.rs) использования.

## P.S.

Уважаемый!
Если вам что-то приглянулось в данном проекте, сочли его полезным, или просто понравился код - не стесняйся поставить ⭐
звездочку в благодарность.
